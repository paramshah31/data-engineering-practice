# About the exercise
1. This exercise reads data from csv files into dataframe.
2. A DDL script is created including PRIMARY & FOREIGN KEYS.
3. We connect to postgres DB hosted on AWS using psycopg2 package.
4. A data schema is created for every table.
5. The data in dataframe is then uploaded to their respective tables.
![Accounts Schema](https://github.com/paramshah31/data-engineering-practice/blob/main/Exercise-5/accounts.png)
![Products Schema](https://github.com/paramshah31/data-engineering-practice/blob/main/Exercise-5/products.png)
![Transaction Schema](https://github.com/paramshah31/data-engineering-practice/blob/main/Exercise-5/transactions.png)
The arrow in transactions schema means they are Foreign Keys.
